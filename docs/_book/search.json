[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intermediate/Advanced Nextflow",
    "section": "",
    "text": "Welcome\nWelcome to our Nextflow workshop for intermediate and advanced users! In this workshop, we will explore the advanced features of the Nextflow language and runtime, and learn how to use them to write efficient and scalable data-intensive workflows. We will cover topics such as parallel execution, error handling, and workflow customization. Please note that this is not an introductory workshop, and we will assume some basic familiarity with Nextflow. By the end of this workshop, you will have the skills and knowledge to create complex and powerful Nextflow pipelines for your own data analysis projects. Let’s get started!"
  },
  {
    "objectID": "operators.html#map",
    "href": "operators.html#map",
    "title": "1  Operator Tour",
    "section": "1.1 map",
    "text": "1.1 map\n\nBasics\nMap is certainly the most commonly used of the operators covered here. It’s a way to supply a closure through which each element in the channel is passed. The return value of the closure is emitted as an element in a new output channel. A canonical example is a closure that multiplies two numbers:\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map { it * it }\n    | view\n}\n\nBy default, the element being passed to the closure is given the default name it. The variable can be named by using the -> notation:\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map { num -> num * num }\n    | view\n}\n\nGroovy is an optionally typed language, and it is possible to specify the type of the argument passed to the closure.\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map { Integer num -> num * num }\n    | view\n}\n\n\n\nNamed Closures\nIf you find yourself re-using the same closure multiple times in your pipeline, the closure can be named and referenced:\n\ndef squareIt = { Integer num -> num * num }\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map( squareIt )\n    | view\n}\n\nIf you have these re-usable closures defined, you can compose them together.\n\ndef squareIt = { it * it }\ndef addTwo = { it + 2 }\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map( squareIt >> addTwo )\n    | view\n}\n\nN E X T F L O W  ~  version 22.10.4\nLaunching `./main.nf` [tender_khorana] DSL2 - revision: f3c3e751fe\n3\n6\n11\n18\n27\n\n\nThe above is the same as writing:\n\ndef squareIt = { it * it }\ndef addTwo = { it + 2 }\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map( squareIt )\n    | map( addTwo )\n    | view\n}\n\nFor those inclined towards functional programming, you’ll be happy to know that closures can be curried:\n\ndef timesN = { multiplier, it -> it * multiplier }\ndef timesTen = timesN.curry(10)\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map( timesTen )\n    | view\n}"
  },
  {
    "objectID": "operators.html#view",
    "href": "operators.html#view",
    "title": "1  Operator Tour",
    "section": "1.2 view",
    "text": "1.2 view\nIn addition to the argument-less usage of view as shown above, this operator can also take a closure to customize the stdout message. We can create a closure to print the value of the elements in a channel as well as their type, for example:\n\ndef timesN = { multiplier, it -> it * multiplier }\ndef timesTen = timesN.curry(10)\ndef prettyPrint = { \"Found '$it' (${it.getClass()})\"}\n\nworkflow {\n    Channel.of( 1, 2, 3, 4, 5 )\n    | map( timesTen )\n    | view( prettyPrint )\n}\n\nN E X T F L O W  ~  version 22.10.4\nLaunching `./main.nf` [goofy_mirzakhani] DSL2 - revision: 987db913a7\nFound '10' (class java.lang.Integer)\nFound '20' (class java.lang.Integer)\nFound '30' (class java.lang.Integer)\nFound '40' (class java.lang.Integer)\nFound '50' (class java.lang.Integer)\n\n\n\n\n\n\n\n\nMost closures will remain anonymous\n\n\n\nIn many cases, it is simply cleaner to keep the closure anonymous, defined inline. Giving closures a name is only recommended when you find yourself defining the same or similar closures repeatedly in a given workflow."
  },
  {
    "objectID": "operators.html#splitcsv",
    "href": "operators.html#splitcsv",
    "title": "1  Operator Tour",
    "section": "1.3 splitCsv",
    "text": "1.3 splitCsv\nIt is common that a samplesheet is passed as input into a Nextflow workflow. We’ll see some more complicated ways to manage these inputs later on in the workshop, but the splitCsv is an excellent tool to have in a pinch.\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header: true )\n    | view\n}\n\n\nExercise\nFrom the directory chapter_01_operators, use the splitCsv and map operators to create a channel that would be suitable input to the\n\nprocess FastQC {\n    input:\n    tuple val(id), path(fastqs)\n    //\n\n\n\nShow answer\n\nSpecifying the header argument in the splitCsv operator, we have convenient named access to csv elements. The closure returns a list of two elements where the second element a list of paths.\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header: true )\n    | map { row -> \n        [row.id, [file(row.fastq1), file(row.fastq2)]]\n    }\n    | view\n}\n\n\n\n\n\n\n\nConvert Strings to Paths\n\n\n\nThe fastq paths are simple strings in the context of a csv row. In order to pass them as paths to a Nextflow process, they need to be converted into objects that adjere to the Path interface. This is accomplished by wrapping them in file.\n\n\nIn the sample above, we’ve lost an important piece of metadata - the tumor/normal classification, choosing only the sample id as the first element in the output list. In the next chapter, we’ll discuss the “meta map” pattern in more detail, but we can preview that here.\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header: true )\n    | map { row -> \n        metaMap = [id: row.id, type: row.type, repeat: row.repeat]\n        [metaMap, [file(row.fastq1), file(row.fastq2)]]\n    }\n    | view\n}\n\nThe construction of this map is very repetitive, and in the next chapter, we’ll discuss some Groovy methods available on the Map class that can make this pattern more concise and less error-prone."
  },
  {
    "objectID": "operators.html#multimap",
    "href": "operators.html#multimap",
    "title": "1  Operator Tour",
    "section": "1.4 multiMap",
    "text": "1.4 multiMap\nThe multiMap operator is a way of creating multiple channels from a single source.\nLet’s assume we’ve been given a samplesheet that has tumor/normal pairs bundled together on the same row.\n\ncd chapter_01_operators\ncat data/samplesheet.ugly.csv\n\nUsing the splitCsv operator would give us one entry that would contain all four fastq files. Let’s consider that we wanted to split these fastqs into separate channels for tumor and normal, we could use multiMap:\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.ugly.csv\")\n    | splitCsv( header: true )\n    | multiMap { row ->\n        tumor: \n            metamap = [id: row.id, type:'tumor', repeat:row.repeat]\n            [metamap, file(row.tumor_fastq_1), file(row.tumor_fastq_2)]\n        normal:\n            metamap = [id: row.id, type:'normal', repeat:row.repeat]\n            [metamap, file(row.normal_fastq_1), file(row.normal_fastq_2)]\n    }\n    | set { samples }\n\n    samples.tumor | view { \"Tumor: $it\"}\n    samples.normal | view { \"Normal: $it\"}\n}\n\n\n\n\n\n\n\nTip: multiMapCriteria\n\n\n\nThe closure supplied to multiMap needs to return multiple channels, so using named closures as described in the map section above will not work. Fortuntely, Nextflow provides the convenience multiMapCriteria method to allow you to define named multiMap closures should you need them. See the multiMap documentation for more info."
  },
  {
    "objectID": "operators.html#branch",
    "href": "operators.html#branch",
    "title": "1  Operator Tour",
    "section": "1.5 branch",
    "text": "1.5 branch\nIn the example above, the multiMap operator was necessary because we were supplied with a samplesheet that combined two pairs of fastq per row. If we were to use the neater samplesheet, we could use the branch operator to achieve the same result.\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header: true )\n    | map { row -> [[id: row.id, repeat: row.repeat, type: row.type], [file(row.fastq1), file(row.fastq2)]] }\n    | branch { meta, reads ->\n        tumor: meta.type == \"tumor\"\n        normal: meta.type == \"normal\"\n    }\n    | set { samples }\n\n    samples.tumor | view { \"Tumor: $it\"}\n    samples.normal | view { \"Normal: $it\"}\n}\n\nAn element is only emitted to the first channel were the test condition is met. If an element does not meet any of the tests, it is not emitted to any of the output channels. You can ‘catch’ any such samples by specifying true as a condition. If we knew that all samples would be either tumor or normal and no third ‘type’, we could write\n\nbranch { meta, reads ->\n    tumor: meta.type == \"tumor\"\n    normal: true\n}\n\nWe can optionally return a new element to one or more of the output channels. For example, to add an extra key in the meta map of the tumor samples, we add a new line under the condition and return our new element. In this example, we modify the first element of the List to be a new list that is the result of merging the existing meta map with a new map containing a single key:\n\nbranch { meta, reads ->\n    tumor: meta.type == \"tumor\"\n        return [meta + [newKey: 'myValue'], reads]\n    normal: true\n}\n\n\nExercise\nHow would you modify the element returned in the tumor channel to have the key:value pair type:'abnormal' instead of type:'tumor'?\n\n\nShow answer\n\nThere are many ways to accomplish this, but the map merging pattern introduced above can also be used to safely and concisely rename values in a map.\n\nbranch { meta, reads ->\n    tumor: meta.type == \"tumor\"\n        return [meta + [type: 'abnormal'], reads]\n    normal: true\n}\n\n\n\n\n\n\n\nMerging maps is safe\n\n\n\nUsing the + operator to merge two or more Maps returns a new Map. There are rare edge cases where modification of map rather than returning a new map can affect other channels. We discuss this further in the next chapter, but just be aware that this + operator is safer and often more convenient than modifying the meta object directly.\nSee the Groovy Map documentation for details.\n\n\n\n\n\nMulti-channel Objects\nSome Nextflow operators return objects that contain multiple channels. The multiMap and branch operators are excellent examples. In most instances, the output is assigned to a variable and then addressed by name:\n\nnumbers = Channel.from(1,2,3,4,5) \n| multiMap {\n    small: it\n    large: it * 10\n}\nnumbers.small | view { num -> \"Small: $num\"}\nnumbers.large | view { num -> \"Large: $num\"}\n\nor by using set:\n\nChannel.from(1, 2, 3, 40, 50) \n| branch {\n    small: it\n    large: it * 10\n}\n| set { numbers }\n\nnumbers.small | view { num -> \"Small: $num\"}\nnumbers.large | view { num -> \"Large: $num\"}\n\nGiven a process that takes multiple channels\n\nprocess MultiInput {\n    debug true\n    input:\n    val(smallNum)\n    val(bigNum)\n\n    \"echo -n small is $smallNum and big is $bigNum\"\n}\n\nYou can either provide the channels individually:\n\nChannel.from(1,2,3,4,5) \n| multiMap {\n    small: it\n    large: it * 10\n}\n| set { numbers }\n\nMultiInput(numbers.small, numbers.large)\n\nor you can provide the multichannel as a single input:\n\nChannel.from(1,2,3,4,5) \n| multiMap {\n    small: it\n    large: it * 10\n}\n| set { numbers }\n\nMultiInput(numbers)\n\nThis also means you can skip the set operator for the cleanest solution:\n\nChannel.from(1,2,3,4,5) \n| multiMap {\n    small: it\n    large: it * 10\n}\n| MultiInput\n\nIf you have processes that output multiple channels and input multiple channels and the cardinality matches, they can be chained together in the same manner."
  },
  {
    "objectID": "operators.html#transpose",
    "href": "operators.html#transpose",
    "title": "1  Operator Tour",
    "section": "1.6 transpose",
    "text": "1.6 transpose\nThe transpose operator is often misunderstood. It can be thought of as the inverse of the groupTuple operator. Give the following workflow, the groupTuple and transpose operators cancel each other out. Removing lines 8 and 9 returns the same result.\nGiven a workflow that returns one element per sample, where we have grouped the samplesheet lines on a meta containing only id and type:\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv(header: true)\n    | map { row -> \n        meta = [id: row.id, type: row.type]\n        [meta, row.repeat, [row.fastq1, row.fastq2]]\n    }\n    | groupTuple\n    | view\n}\n\nN E X T F L O W  ~  version 22.10.4\nLaunching `./main.nf` [cheesy_jones] DSL2 - revision: 7dc1cc0039\n[[id:sampleA, type:normal], [1, 2], [[data/reads/sampleA_rep1_normal_R1.fastq.gz, data/reads/sampleA_rep1_normal_R2.fastq.gz], [data/reads/sampleA_rep2_normal_R1.fastq.gz, data/reads/sampleA_rep2_normal_R2.fastq.gz]]]\n[[id:sampleA, type:tumor], [1, 2], [[data/reads/sampleA_rep1_tumor_R1.fastq.gz, data/reads/sampleA_rep1_tumor_R2.fastq.gz], [data/reads/sampleA_rep2_tumor_R1.fastq.gz, data/reads/sampleA_rep2_tumor_R2.fastq.gz]]]\n[[id:sampleB, type:normal], [1], [[data/reads/sampleB_rep1_normal_R1.fastq.gz, data/reads/sampleB_rep1_normal_R2.fastq.gz]]]\n[[id:sampleB, type:tumor], [1], [[data/reads/sampleB_rep1_tumor_R1.fastq.gz, data/reads/sampleB_rep1_tumor_R2.fastq.gz]]]\n[[id:sampleC, type:normal], [1], [[data/reads/sampleC_rep1_normal_R1.fastq.gz, data/reads/sampleC_rep1_normal_R2.fastq.gz]]]\n[[id:sampleC, type:tumor], [1], [[data/reads/sampleC_rep1_tumor_R1.fastq.gz, data/reads/sampleC_rep1_tumor_R2.fastq.gz]]]\n\n\nIf we add in a transpose, each repeat number is matched back to the appropriate list of reads:\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv(header: true)\n    | map { row -> \n        meta = [id: row.id, type: row.type]\n        [meta, row.repeat, [row.fastq1, row.fastq2]]\n    }\n    | groupTuple\n    | transpose\n    | view\n}\n\nN E X T F L O W  ~  version 22.10.4\nLaunching `./main.nf` [loving_lichterman] DSL2 - revision: 2c5476b133\n[[id:sampleA, type:normal], 1, [data/reads/sampleA_rep1_normal_R1.fastq.gz, data/reads/sampleA_rep1_normal_R2.fastq.gz]]\n[[id:sampleA, type:normal], 2, [data/reads/sampleA_rep2_normal_R1.fastq.gz, data/reads/sampleA_rep2_normal_R2.fastq.gz]]\n[[id:sampleA, type:tumor], 1, [data/reads/sampleA_rep1_tumor_R1.fastq.gz, data/reads/sampleA_rep1_tumor_R2.fastq.gz]]\n[[id:sampleA, type:tumor], 2, [data/reads/sampleA_rep2_tumor_R1.fastq.gz, data/reads/sampleA_rep2_tumor_R2.fastq.gz]]\n[[id:sampleB, type:normal], 1, [data/reads/sampleB_rep1_normal_R1.fastq.gz, data/reads/sampleB_rep1_normal_R2.fastq.gz]]\n[[id:sampleB, type:tumor], 1, [data/reads/sampleB_rep1_tumor_R1.fastq.gz, data/reads/sampleB_rep1_tumor_R2.fastq.gz]]\n[[id:sampleC, type:normal], 1, [data/reads/sampleC_rep1_normal_R1.fastq.gz, data/reads/sampleC_rep1_normal_R2.fastq.gz]]\n[[id:sampleC, type:tumor], 1, [data/reads/sampleC_rep1_tumor_R1.fastq.gz, data/reads/sampleC_rep1_tumor_R2.fastq.gz]]"
  },
  {
    "objectID": "metadata.html#metadata-import",
    "href": "metadata.html#metadata-import",
    "title": "2  Metadata Propagation",
    "section": "2.1 Metadata Import",
    "text": "2.1 Metadata Import\nIdeally, you have sample information stored in a simple and structured samplesheet. These are often files in CSV/TSV format or similar. I’d like to start with a worst-case scenario where you’ve been handed a bag of files that you need to make sense of. We’ll use this example to introduce some helpful Groovy syntactic sugar and features that will be helpful in other Nextflow contexts.\nGiven a bag of fastq reads:\n\ncd chapter_02_metadata\ntree data/reads\n\ndata/reads/\n|-- treatmentA\n|   |-- sampleA_rep1_normal_R1.fastq.gz\n|   |-- sampleA_rep1_normal_R2.fastq.gz\n|   |-- sampleA_rep1_tumor_R1.fastq.gz\n|   |-- sampleA_rep1_tumor_R2.fastq.gz\n|   |-- sampleA_rep2_normal_R1.fastq.gz\n|   |-- sampleA_rep2_normal_R2.fastq.gz\n|   |-- sampleA_rep2_tumor_R1.fastq.gz\n|   |-- sampleA_rep2_tumor_R2.fastq.gz\n|   |-- sampleB_rep1_normal_R1.fastq.gz\n|   |-- sampleB_rep1_normal_R2.fastq.gz\n|   |-- sampleB_rep1_tumor_R1.fastq.gz\n|   |-- sampleB_rep1_tumor_R2.fastq.gz\n|   |-- sampleC_rep1_normal_R1.fastq.gz\n|   |-- sampleC_rep1_normal_R2.fastq.gz\n|   |-- sampleC_rep1_tumor_R1.fastq.gz\n|   `-- sampleC_rep1_tumor_R2.fastq.gz\n`-- treatmentB\n    |-- sampleA_rep1_normal_R1.fastq.gz\n    |-- sampleA_rep1_normal_R2.fastq.gz\n    |-- sampleA_rep1_tumor_R1.fastq.gz\n    `-- sampleA_rep1_tumor_R2.fastq.gz\n\n\n\n\n\n\nWarning\n\n\n\nWhomever has handed us these files has encoded metadata in both the filename, but also the name of the parent directories treatmentA and treatmentB.\n\n\n\nFirst Pass\nA first pass attempt at pulling these files into Nextflow might use the fromFilePairs method:\n\nworkflow {\n    Channel.fromFilePairs(\"data/reads/*/*_R{1,2}.fastq.gz\")\n    | view\n}\n\nNextflow will pull out the first part of the fastq filename and returned us a channel of tuple elements where the first element is the filename-derived ID and the second element is a list of two fastq files.\nThe id is stored as a simple string. We’d like to move to using a map of key-value pairs because we have more than one piece of metadata to track. In this example, we have sample, replicate, tumor/normal, and treatment. We could add extra elements to the tuple, but this changes the ‘cardinality’ of the elements in the channel and adding extra elements would require updating all downstream processes. A map is a single object and is passed through Nextflow channels as one value, so adding extra metadata fields will not require us to change the cardinality of the downstream processes.\nThere are a couple of different ways we can pull out the metadata\nWe can use the tokenize method to split our id. To sanity-check, I just pipe the result directly into the view operator.\n\nworkflow {\n    Channel.fromFilePairs(\"data/reads/*/*_R{1,2}.fastq.gz\")\n    | map { id, reads ->\n        tokens = id.tokenize(\"_\")\n    }\n    | view\n}\n\nIf we are confident about the stability of the naming scheme, we can destructure the list returned by tokenize and assign them to variables directly:\n\nmap { id, reads ->\n    (sample, replicate, type) = id.tokenize(\"_\")\n    meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n\n\n\n\n\n\n\nDestructuring requires parentheses\n\n\n\nMake sure that you’re using a tuple with parentheses e.g. (one, two) rather than a List e.g. [one, two]\n\n\nAnother option is to use the transpose method with the collectEntries() to produce the same map. I’d warn that this method is bordering on a little ‘too clever’ and is more difficult to read. It also assumes that the order of the filename-encoded metadata is consistent.\n\nmap { id, reads ->\n    meta = [id.tokenize(\"_\"), ['sample', 'replicate', 'type']]\n        .transpose()\n        .collectEntries()\n    [meta, reads]\n}\n\nIf we move back to the previous method, but decided that the ‘rep’ prefix on the replicate should be removed, we can use regular expressions to simply “subtract” pieces of a string. Here we remove a ‘rep’ prefix from the replicate variable if the prefix is present:\n\nmap { id, reads ->\n    (sample, replicate, type) = id.tokenize(\"_\")\n    replicate -= ~/^rep/\n    meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n\n\n\n\n\n\n\nTrim strings\n\n\n\nGroovy has a lot of very helpful syntactic sugar for string manipulation. You can trim parts of a string by simply subtracting another string:\n\ndemo = \"one two three\"\nassertEquals(demo - \"two \", \"one three\")\n\n… or by subtracting a regular expression:\n\ndemo = \"one two three\"\nassertEquals(demo - ~/t.o ?/, \"one three\")\n\nTo quickly sanity-check a groovy expression, try the Groovy web console\n\n\nWe are almost there, but the we still don’t have the “treatment” metadata captured in our meta map. The treament is encoded in this example in the name of the parent directory relative to the reads. Inside the map object, the reads are a list of two UnixPath objects. These objects implement the java.nio.Path interface, which provides us many useful methods, including getParent().\nWe can call the getParent() method on each of the paths like so:\n\nmap { id, reads ->\n    reads.collect { it.getParent() }\n}\n\nIf we want to call a set method on every item in a Collection, Groovy provides this convenient “spread dot” notation:\n\nmap { id, reads ->\n    reads*.getParent()\n}\n\nthis returns another Path object, but we only want the name of the last directory, so we need to call .getName() method on each of these Paths. We can use the spread-dot notation again:\n\nmap { id, reads ->\n    reads*.getParent()*.getName()\n}\n\nThe last piece of Groovy sugar is to note that methods with get and set prefixes can be called with a property-style notation, converting getParent() to parent and getName() to name:\n\nmap { id, reads ->\n    reads*.parent*.name\n}\n\nIf we wanted to remove the “treatment” prefix, we can combine this new notation with the “minus” method which we used earlier in the aliased - form.\n\nmap { id, reads ->\n    reads*.parent*.name*.minus(~/treatment/)\n}\n\nIn this particular example, we know ahead of time that the treatments must be the same because of the way the fromFilePairs method gathers pairs, but we’ll continue for the sake of the demonstration. Our final map closure might look like:\n\nworkflow {\n    Channel.fromFilePairs(\"data/reads/*/*_R{1,2}.fastq.gz\")\n    | map { id, reads ->\n        (sample, replicate, type) = id.tokenize(\"_\")\n        (treatmentFwd, treatmentRev) = reads*.parent*.name*.minus(~/treatment/)\n        meta = [\n            sample:sample,\n            replicate:replicate,\n            type:type,\n            treatmentFwd:treatmentFwd,\n            treatmentRev:treatmentRev,\n        ]\n        [meta, reads]\n    }\n    | view\n}\n\nThis metadata map can be passed through the workflow with the reads and used to split, join and recombine the data. The resulting channel would be suitable for any Nextflow process with inputs of the form\n\nprocess ExampleProcess {\n    input:\n    tuple val(meta), path(reads)\n\n    // ...\n\nThis channel “shape” or cardinarlity is extremely common in nf-core modules and subworkflows and is critical to enabling reusability of these modules."
  },
  {
    "objectID": "grouping.html#grouping-using-submap",
    "href": "grouping.html#grouping-using-submap",
    "title": "3  Grouping and Splitting",
    "section": "3.1 Grouping using subMap",
    "text": "3.1 Grouping using subMap\nNow that we have a channel that conforms to the tuple val(meta) path(<something>) pattern, we can investigate splitting and grouping patterns.\nWe’ll start with a simple main.nf in the chapter_03_grouping directory\n\ncd chapter_03_grouping\n\n\n\n\nmain.nf\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header:true )\n    | map { row ->\n        meta = [id:row.id, repeat:row.repeat, type:row.type]\n        [meta, [\n            file(row.fastq1, checkIfExists: true), \n            file(row.fastq2, checkIfExists: true)]]\n    }\n    | view\n}\n\n\nThe first change we’re going to make is to correct some repetitive code that we’ve seen quite a lot already in this workshop. The construction of the meta map from this row stutters quite a lot. We can make use of the subMap method available Maps to quickly return a new map constructed from the subset of an existing map:\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header:true )\n    | map { row ->\n        meta = row.subMap('id', 'repeat', 'type')\n        [meta, [\n            file(row.fastq1, checkIfExists: true), \n            file(row.fastq2, checkIfExists: true)]]\n    }\n    | view\n}\n\n\n\n\n\n\n\nComplete meta map safety\n\n\n\nThe subMap method will take a collection of keys and construct a new map with just the keys listed in the collection. This method, in combination with the plus or + method for combining maps and resetting values should allow all contraction, expansion and modification of maps safely.\n\n\n\nExercise\nCan you extend our workflow in an unsafe manner? Use the set operator to name the channel in our workflow above, and then map (the operator) over that without modification. In a separate map operation, try modifying the meta map in a way that is reflected in the first map.\nNote that we’re trying to do the wrong thing in this example to clarify what the correct approach might be.\n\n\nShow answer\n\nTo ensure that the modification of the map happens first, we introduce a sleep into the first map operation. This sleep emulates a long-running Nextflow process.\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header:true )\n    | map { row ->\n        meta = row.subMap('id', 'repeat', 'type')\n        [meta, [\n            file(row.fastq1, checkIfExists: true),\n            file(row.fastq2, checkIfExists: true)]]\n    }\n    | set { samples }\n\n    samples\n    | map { sleep 10; it }\n    | view { meta, reads -> \"Should be unmodified: $meta\" }\n    \n    samples\n    | map { meta, reads ->\n        meta.type = meta.type == \"tumor\" ? \"abnormal\" : \"normal\"\n        [meta, reads]\n    }\n    | view { meta, reads -> \"Should be modified: $meta\" }\n}\n\n\n\n\nExercise\nHow would you fix the example above to use the safe operators plus and subMap to ensure that the original map remains unmodified?\n\n\nShow answer\n\n\nworkflow {\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header:true )\n    | map { row ->\n        meta = row.subMap('id', 'repeat', 'type')\n        [meta, [\n            file(row.fastq1, checkIfExists: true),\n            file(row.fastq2, checkIfExists: true)]]\n    }\n    | set { samples }\n\n    samples\n    | map { sleep 10; it }\n    | view { meta, reads -> \"Should be unmodified: $meta\" }\n    \n    samples\n    | map { meta, reads ->\n        newmap = [type: meta.type == \"tumor\" ? \"abnormal\" : \"normal\"]\n        [meta + newmap, reads]\n    }\n    | view { meta, reads -> \"Should be modified: $meta\" }\n}"
  },
  {
    "objectID": "grouping.html#passing-maps-through-processes",
    "href": "grouping.html#passing-maps-through-processes",
    "title": "3  Grouping and Splitting",
    "section": "3.2 Passing maps through processes",
    "text": "3.2 Passing maps through processes\nLet’s construct a dummy read mapping process. This is not a bioinformatics workshop, so we can ‘cheat’ in the interests of time.\n\nprocess MapReads {\n    input:\n    tuple val(meta), path(reads)\n    path(genome)\n\n    output:\n    tuple val(meta), path(\"*.bam\")\n\n    \"touch out.bam\"\n}\n\nworkflow {\n    reference = Channel.fromPath(\"data/genome.fasta\").first()\n\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header:true )\n    | map { row ->\n        meta = row.subMap('id', 'repeat', 'type')\n        [meta, [\n            file(row.fastq1, checkIfExists: true), \n            file(row.fastq2, checkIfExists: true)]]\n    }\n    | set { samples }\n    \n    MapReads( samples, reference )\n    | view\n}\n\nLet’s consider that we might now want to merge the repeats. We’ll need to group bams that share the id and type attributes.\n\nMapReads( samples, reference )\n| map { meta, bam -> [meta.subMap('id', 'type'), bam]}\n| groupTuple\n| view\n\nThis is easy enough, but the groupTuple operator has to wait until all items are emitted from the incoming queue before it is able to reassemble the output queue. If even one read mapping job takes a long time, the processing of all other samples is held up. We need a way of signalling to nextflow how many items are in a given group so that items can be emitted as early as possible.\nBy default, the groupTuple operator groups on the first item in the element, which at the moment is a Map. We can turn this map into a special class using the groupKey method, which takes our grouping object as a first parameter and the number of expected elements in the second parameter.\n\nMapReads( samples, reference )\n| map { meta, bam -> \n    key = groupKey(meta.subMap('id', 'type'), NUMBER_OF_ITEMS_IN_GROUP)\n    [key, bam]\n}\n| groupTuple\n| view \n\n\nExercise\nHow might we modify the upstream channels to the number of repeats into the metamap?\n\n\nShow answer\n\n\nworkflow {\n    reference = Channel.fromPath(\"data/genome.fasta\").first()\n\n    Channel.fromPath(\"data/samplesheet.csv\")\n    | splitCsv( header:true )\n    | map { row ->\n        meta = row.subMap('id', 'repeat', 'type')\n        [meta, [file(row.fastq1, checkIfExists: true), file(row.fastq2, checkIfExists: true)]]\n    }\n    | map { meta, reads -> [meta.subMap('id', 'type'), meta.repeat, reads] }\n    | groupTuple\n    | map { meta, repeats, reads -> [meta + [repeatcount:repeats.size()], repeats, reads] }\n    | transpose\n    | map { meta, repeat, reads -> [meta + [repeat:repeat], reads]}\n    | set { samples }\n    \n    MapReads( samples, reference )\n    | map { meta, bam -> \n        key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n        [key, bam]\n    }\n    | groupTuple\n    | view \n}\n\n\nNow that we have our repeats together in an output channel, we can combine them using advanced bioinformatics<84>\n\nprocess CombineBams {\n    input:\n    tuple val(meta), path(\"input/in_*_.bam\")\n\n    output:\n    tuple val(meta), path(\"combined.bam\")\n\n    \"cat input/*.bam > combined.bam\"\n} \n\nIn our workflow:\n\nMapReads( samples, reference )\n| map { meta, bam -> \n    key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n    [key, bam]\n}\n| groupTuple\n| CombineBams"
  },
  {
    "objectID": "grouping.html#fanning-out-over-intervals",
    "href": "grouping.html#fanning-out-over-intervals",
    "title": "3  Grouping and Splitting",
    "section": "3.3 Fanning out over intervals",
    "text": "3.3 Fanning out over intervals\nThe previous exercise demonstrated the fan-in approach using groupTuple and groupKey, but we might want to fan out our processes. An example might be computing over some intervals - genotyping over intervals, for example.\nWe can take an existing bed file, for example and turn it into a channel of Maps.\n\nChannel.fromPath(\"data/intervals.bed\") \n| splitCsv(header: ['chr', 'start', 'stop', 'name'], sep: '\\t')\n| collectFile { entry -> [\"${entry.name}.bed\", entry*.value.join(\"\\t\")] }\n| view\n| set { intervals }\n\nreturn\n\n\n\n\n\n\n\nQuick return\n\n\n\nIn the example above, I add a return statement for quick debugging. This ensures that all workflow operations after the return are not included in the process DAG.\n\n\nGiven a dummy genotyping process\n\nprocess GenotypeOnInterval {\n    input:\n    tuple val(meta), path(bam), path(bed)\n\n    output:\n    tuple val(meta), path(\"genotyped.bam\")\n\n    \"cat $bam $bed > genotyped.bam\"\n}\n\nWe can use the combine operator to emit a new channel where each combined bam is attached to each bed file. These can then be piped into the genotyping process:\n\nMapReads( samples, reference )\n| map { meta, bam -> \n    key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n    [key, bam]\n}\n| groupTuple\n| CombineBams\n| combine( intervals )\n| GenotypeOnInterval\n| view \n\nFinally, we can combine these genotyped bams back using groupTuple and another bam merge process:\n\nprocess MergeGenotyped {\n    input:\n    tuple val(meta), path(\"input/in_*_.bam\")\n\n    output:\n    tuple val(meta), path(\"merged.genotyped.bam\")\n\n    \"cat input/*.bam > merged.genotyped.bam\"\n}\n\n\nMapReads( samples, reference )\n| map { meta, bam -> \n    key = groupKey(meta.subMap('id', 'type'), meta.repeatcount)\n    [key, bam]\n}\n| groupTuple\n| CombineBams\n| map { meta, bam -> [meta.subMap('id', 'type'), bam] }\n| combine( intervals )\n| GenotypeOnInterval\n| groupTuple\n| MergeGenotyped\n| view"
  },
  {
    "objectID": "groovy.html",
    "href": "groovy.html",
    "title": "4  Groovy Imports",
    "section": "",
    "text": "There exists in Groovy a wealth of helper classes that can be imported into Nextflow scripts. In this chapter, we create a very small Workflow using the FastP tool to investigate importing the Groovy JSONSlurper class.\nFirst, let’s move into the chapter 4 directory:\n\ncd chapter_04_groovy\n\nLet’s assume that we would like to pull in a samplesheet, parse the entries and run them through the FastP tool. So far, we have been concerned with local files, but Nextflow will handle remote files transparently:\n\nworkflow {\n    params.input = \"https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/samplesheet/v3.4/samplesheet_test.csv\"\n\n    Channel.fromPath(params.input)\n    | splitCsv(header: true)\n    | view\n}\n\nLet’s write a small closure to parse each row into the now-familiar map + files shape. We might start by constructing the meta-map:\n\nworkflow {\n    params.input = \"https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/samplesheet/v3.4/samplesheet_test.csv\"\n\n    Channel.fromPath(params.input)\n    | splitCsv(header: true)\n    | map { row ->\n        meta = row.subMap('sample', 'strandedness')\n        meta\n    }\n    | view\n}\n\n… but this precludes the possibility of adding additional columns to the samplesheet. We might to ensure the parsing will capture any extra metadata columns should they be added. Instead, let’s partition the column names into those that begin with “fastq” and those that don’t:\n\n(readKeys, metaKeys) = row.keySet().split { it =~ /^fastq/ }\n\n\n\n\n\n\n\nNew methods\n\n\n\nWe’ve introduced a new keySet method here. This is a method on Java’s LinkedHashMap class (docs here)\nWe’re also using the .split() method, which divides collection based on the return value of the closure. The mrhaki blog provides a succinct summary.\n\n\nFrom here, let’s\n\nreads = row.subMap(readKeys).values().collect { file(it) }\n\n… but we run into an error:\n\nArgument of `file` function cannot be empty\n\nIf we have a closer look at the samplesheet, we notice that not all rows have two read pairs. Let’s add a condition\n\nreads = row\n.subMap(readKeys)\n.values()\n.findAll { it != \"\" } // Single-end reads will have an empty string\n.collect { file(it) } // Turn those strings into paths\n\nNow we need to construct the meta map. Let’s have a quick look at the FASTP module that I’ve already pre-defined:\n\nprocess FASTP {\n    container 'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0'\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.fastp.fastq.gz') , optional:true, emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n\n    script:\n    def prefix = task.ext.prefix ?: meta.id\n    if (meta.single_end) {\n        // SNIP\n    } else {\n        // SNIP\n    }\n\nI can see that we require two extra keys, id and single_end:\n\nmeta = row.subMap(metaKeys)\nmeta.id ?= meta.sample\nmeta.single_end = reads.size == 1\n\nThis is now able to be passed through to our FASTP process:\n\nChannel.fromPath(params.input)\n| splitCsv(header: true)\n| map { row ->\n    (readKeys, metaKeys) = row.keySet().split { it =~ /^fastq/ }\n    reads = row.subMap(readKeys).values()\n    .findAll { it != \"\" } // Single-end reads will have an empty string\n    .collect { file(it) } // Turn those strings into paths\n    meta = row.subMap(metaKeys)\n    meta.id ?= meta.sample\n    meta.single_end = reads.size == 1\n    [meta, reads]\n}\n| FASTP\n\nFASTP.out.json | view\n\nLet’s assume that we want to pull some information out of these JSON files. To make our lives a little more convenient, let’s “publish” these json files so that they are more convenient. We’re going to discuss configuration more completely in a later chapter, but that’s no reason not to dabble a bit here.\nWe’d like to add a publishDir directive to our FASTP process.\n\nprocess {\n    withName: 'FASTP' {\n        publishDir = [\n            path: { \"results/fastp/json\" },\n            saveAs: { filename -> filename.endsWith('.json') ? filename : null },\n        ]\n    }\n}\n\n\n\n\n\n\n\nGroovy Tip: Elvis Operator\n\n\n\nThis pattern of returning something if it is true and somethingElse if not:\n\nsomethingThatMightBeFalsey ? somethingThatMightBeFalsey : somethingElse\n\nhas a shortcut in Groovy - the “Elvis” operator:\n\nsomethingThatMightBeFalsey ?: somethingElse\n\n\n\nThis enables us to iterate quickly to test out our JSON parsing without waiting on the FASTP caching to calculate on these slow virtual machines.\n\nnextflow run . -resume\n\nLet’s consider the possibility that we’d like to capture some of these metrics so that they can be used downstream. First, we’ll have a quick peek at the Groovy docs and I see that I need to import a JsonSlurper:\n\nimport groovy.json.JsonSlurper\n\n// We can also import a Yaml parser in the same manner if required:\n// import org.yaml.snakeyaml.Yaml\n// new Yaml().load(new FileReader('your/data.yml'))\n\nNow let’s create a second entrypoint to quickly pass these JSON files through some tests:\n\n\n\n\n\n\nEntrypoint developing\n\n\n\nUsing a second Entrypoint allows us to do quick debugging or development using a small section of the workflow without disturbing the main flow.\n\n\n\nworkflow Jsontest {\n    Channel.fromPath(\"results/fastp/json/*.json\")\n    | view\n}\n\nwhich we run with\n\nnextflow run . -resume -entry Jsontest\n\nLet’s create a small function at the top of the workflow to take the JSON path and pull out some basic metrics:\n\ndef getFilteringResult(json_file) {\n    fastpResult = new JsonSlurper().parseText(json_file.text)\n}\n\n\nExercise\nThe fastpResult returned from the parseText method is a large Map - a class which we’re already familiar with. Modify the getFilteringResult function to return just the after_filtering section of the report.\n\n\nReveal answer\n\nHere is one potential solution.\n\ndef getFilteringResult(json_file) {\n    new JsonSlurper().parseText(json_file.text)\n    ?.summary\n    ?.after_filtering\n}\n\n\n\n\n\n\n\nNew notation: ?.\n\n\n\nThis new notation is a null-safe access operator. The ?.summary will access the summary property if the property exists.\n\n\n\nWe can then join this new map back to the original reads using the join operator:\n\nFASTP.out.json \n| map { meta, json -> [meta, getFilteringResult(json)] }\n| join( FASTP.out.reads )\n| view\n\n\n\nExercise\nCan you amend this pipeline to create two channels that filter the reads to exclude any samples where the Q30 rate is less than 93.5%\n\n\nReveal answer\n\n\nFASTP.out.json \n| map { meta, json -> [meta, getFilteringResult(json)] }\n| join( FASTP.out.reads )\n| map { meta, fastpMap, reads -> [meta + fastpMap, reads] }\n| branch { meta, reads ->\n    pass: meta.q30_rate >= 0.935\n    fail: true\n}\n| set { reads }\n\nreads.fail | view { meta, reads -> \"Failed: ${meta.id}\" }\nreads.pass | view { meta, reads -> \"Passed: ${meta.id}\" }"
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "5  Workflow Structure",
    "section": "",
    "text": "To be introduced on day #2"
  },
  {
    "objectID": "configuration.html#precedence",
    "href": "configuration.html#precedence",
    "title": "6  Configuration",
    "section": "6.1 Precedence",
    "text": "6.1 Precedence\n\nParameters specified on the command line (–something value)\nParameters provided using the -params-file option\nConfig file specified using the -c my_config option\nThe config file named nextflow.config in the current directory\nThe config file named nextflow.config in the workflow project directory\nThe config file $HOME/.nextflow/config\nValues defined within the pipeline script itself (e.g. main.nf)"
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "7  Troubleshooting",
    "section": "",
    "text": "To be introduced on day #2"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "Summary available on day #2"
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "9  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]
